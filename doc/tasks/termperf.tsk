
Tue May 10 08:59:35 EDT 2011

Task: termer performance is really really slow.

I should run profiling to figure out what's going on. I have some ideas
though...

- Updating whole display is slow?
    Perhaps it's just always updating the whole screen which is slow. Maybe if
    we only update those areas which change we won't have so much of a
    problem.

    If this were case, I would expect a much bigger terminal size would take
    longer to render the same thing as a much smaller terminal size.

- Rendering characters is slow  
    Maybe we need to cache glyphs.

- consoler is fundamentally slow?
    Like, all this copying stuff we do...


Let me turn on profiling of the haskell code, run the time profile, and see
what it says after cating a bunch of stuff to the screen.

Tue May 10 09:09:37 EDT 2011

Well, that's pretty clear. Profiling says 80% of the time is being spent in
the showDisplay command. Notice, this is not the draw cell command.

I must say, I find that a little surprising. Is it really so much work to copy
a buffer to a pipe?

Unless the issue is sdlcsr is slow to read the display data, so it's getting
backed up and the update is blocking until there's enough space in the pipe.
Or maybe we can only send a little bit of a data at a time over the fifo, and
we're trying to send 640 * 480 = 307200 bytes at once, or 75 pages. That could
definately be a problem.

This suggests to me the problem could be with using pipes for communication.
That might also explain why we have performance issues with pdfer updating
the display.

I would like to profile sdlscr too. See where it's spending all its time.

Anyway, the interesting things to take away from this profile run are:
 - rendering characters doesn't seem to be the problem
 - constant communication between haskell and C doesn't seem to be a problem
 - The problem is somewhere between sending the updated display and showing
   the updated display.

Let's do sdlscr profiling next. That will hopefuly tell us if the problem is
in drawing the updated display, or waiting for the updated display.

Tue May 10 09:28:52 EDT 2011

As with past attempts, I find profiling sdl doesn't work. It doesn't give me
any time info. Gar! That's so annoying.

The one thing it does say is we can CNSL_GetRed and friends a lot. That is, we
do lots of deconstruction and construction of the colors, which is surely
almost entirely wasted.

The major problem I'm facing here is lack of insight as to what's going on in
my programs. gprof seems not to work so well for sdl programs, and profiling
haskell is always hard for some reason.

Anyway, based on my incomplete information now, I would recommend the
following changes to improve the performance of the termer:
 - Only update that part of the display which has changed
    for my test case usually that's just one cell instead of the whole screen.
    I would expect a speedup of something like 300x then, because we are just
    dealing with so many less pixels.

    This will need work both in ctermer to not update the whole display, and
    in Termer.hs, to not update all the characters.

 - In sdlcsr don't deconstruct and reconstruct the color, make an efficient
   function for converting CNSL colors to sdlcsr colors, namely: the identity
   function.

But the real solution is to figure out how to profile sdl code.

Wed May 11 20:45:27 EDT 2011

The first parts hard. Let me...

Okay, this is sad, but I need to really write a test first of the
communication link.

Here's what I propose. Write an application which switches back and forth
between black and white and black and white. Record how long it takes to do
that for various screen dimensions to try and get an estimate on throughput.

Now, once I have that, make the second optimization with the colors thing, see
if it makes any difference.

This test will be useful for trying out other optimizations to the
communication protocol.

time ./build/src/sdlcsr ./build/src/flicker 640 480 100
hw_available: 0
wm_available: 1

real    0m32.695s
user    0m4.539s
sys     0m26.168s



richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 1280 800
hw_available: 0
wm_available: 1

real    1m35.288s
user    0m14.970s
sys     0m56.615s


time ./build/src/sdlcsr ./build/src/flicker 640 480 100
hw_available: 1
wm_available: 0

real    0m35.451
user    0m11.224
sys     0m24.272

richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 1280 800
hw_available: 1
wm_available: 0

real    1m58.776s
user    0m35.857s
sys     1m13.976s

Some initial results in X and not in X of various sizes.
Hardware acceleration actually appears to slow us down. Or X is speeding us up
or some such.


Now let me try my performance optimization: don't deconstruct and reconstruct
colors. See what that all does to things.

richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 1280 800 100
in X
real    1m54.808s
user    0m12.093s
sys     1m40.698s
 
richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 640 480 100
in X

real    0m27.543s
user    0m4.375s
sys     0m22.548s


That didn't make much of a difference.

Sigh. So I have other ideas, such as:
  - fifos are the problem
  - using FillRect instead of just setting pixels directly.

But the right way to do this has got to be enable profiling. So I really
should figure out next how to do that.

Certainly we can reduce the amount of the terminal we update, but for focus in
general, just imagine you are scrolling the screen or a pdf, the whole page
will need to be updated, so we should figure out some way of making that be
fast.

Wed May 11 21:15:54 EDT 2011

Part of the problem is enabling, disabling profiling. Perhaps it is time to
separate consoler out into a library, then start building things with and
without profiling enabled?

I can at least enable profiling on specific applications.

Wed May 11 21:29:45 EDT 2011

Read online about gprof issues. The multithreaded problem with gprof is not
what I'm encountering, because even then we should still be profiling the main
thread. I'm seeing nothing there.

The mac OS X problem is not what I'm encoutering, because I don't have a mac.

The not run long enough problem is not what I'm encountering, because I've run
it for a long long time.

Other things to check: 
 - do we terminate cleanly? If not, we might not get useful output, and I've
   heard sdl might fail to terminate cleanly with gprof.
    No, we are definately exiting cleanly...

 - do non-sdl programs have the same problem? For example, what if I just run
   flicker built with profiling enabled?

    Yes, this works.

So what is sdl doing to break profiling? Probably mucking with timers or some
such.

Anyway, an interesting thing to note: profiling just flicker shows that
flicker is really slow to run by itself.

  %   cumulative   self              self     total           
 time   seconds   seconds    calls  Ts/call  Ts/call  name    
 51.87      0.57     0.57                             CNSL_SendDisplay
 42.77      1.04     0.47                             CNSL_GetPixel
  2.73      1.07     0.03                             CNSL_FreeDisplay
  1.82      1.09     0.02                             CNSL_SetPixel
  0.91      1.10     0.01                             main

The profile suggests that CNSL_SendDisplay and CNSL_GetPixel are slow. I guess
we can't really understand the meaning of that because we don't know that the
interpreter of the data isn't taking a long time. Hmm... what if we pipe the
output to /dev/null? That would assume the server can suck out as much data as
possible, and test how fast flicker can deliver the data. This is an
interesting question.

The answer:

width: 640
height: 480
frames: 100

real    0m7.774s
user    0m2.009s
sys     0m5.762s

Which means... a while.

Another thing I could do: start sdlcsr, and watch top. See which process is
flooring the cpu, which is waiting, and so on.

Top shows sdlcsr as taking 99% cpu while flicker takes 95% cpu. The whole
thing takes about 30 seconds. Not sure if that means anything.

All of which suggests... I'm not sure. That the communication link is slow
certainly...

Well, if I'm really concerned, I could try implementng an alternate version of
ctermer. One whose implementation is directly in SDL instead of using
consoler. If the performance of that is fine, I'll know SDL isn't a big
bottleneck. If the performance of that sucks, perhaps SDL is a problem.

Of course, fbterm is pure SDL and it doesn't have any problems...

I should run this experiment.

Thu May 12 17:01:05 EDT 2011

I made an implementation of termer which uses SDL directly instead of
consoler. It goes noticably faster, but it's still noticably slow.

Which suggests there are many performance issues I'll have to deal with.

Maybe it makes sense to get this version fast before adding the consoler
stuff.

I have the feeling doing FillRect to draw a pixel in SDL is bad. And I think
updating the entire screen for a single character is bad. I propose fixing
those two things to be the next steps in sdltermer.

Unfortunately, it doesn't make sense to do that before I have some sort of way
to evaluate how fast those changes are. I need a benchmark for the terminal
emulator, some nice way to measure how fast it is.

Well... the program I run could be different. Instead of running sh, I could
run a script, it outputs a bunch of stuff (no input), time how long it takes.
That sounds good to me.

Thu May 12 17:17:09 EDT 2011

I made a little test script. It just echos a bunch of stuff to the screen.

On 640x480 pixels, 40x12 lines, fontsize 32, pacific with sdltermer:
 It takes 18 seconds to run.

In contrast, the amount of time it takes to run using sdlcsr...


7m03 seconds. Wow. That's pretty big. 23x

So yes, consoler definately is costly. And now I can do my performance
measurements.

Just for reference, fbcon gets .029s, so I've got a ways to go. Especially
considering fbcon is something I would consider slow.


