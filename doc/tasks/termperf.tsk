
Tue May 10 08:59:35 EDT 2011

Task: termer performance is really really slow.

I should run profiling to figure out what's going on. I have some ideas
though...

- Updating whole display is slow?
    Perhaps it's just always updating the whole screen which is slow. Maybe if
    we only update those areas which change we won't have so much of a
    problem.

    If this were case, I would expect a much bigger terminal size would take
    longer to render the same thing as a much smaller terminal size.

- Rendering characters is slow  
    Maybe we need to cache glyphs.

- consoler is fundamentally slow?
    Like, all this copying stuff we do...


Let me turn on profiling of the haskell code, run the time profile, and see
what it says after cating a bunch of stuff to the screen.

Tue May 10 09:09:37 EDT 2011

Well, that's pretty clear. Profiling says 80% of the time is being spent in
the showDisplay command. Notice, this is not the draw cell command.

I must say, I find that a little surprising. Is it really so much work to copy
a buffer to a pipe?

Unless the issue is sdlcsr is slow to read the display data, so it's getting
backed up and the update is blocking until there's enough space in the pipe.
Or maybe we can only send a little bit of a data at a time over the fifo, and
we're trying to send 640 * 480 = 307200 bytes at once, or 75 pages. That could
definately be a problem.

This suggests to me the problem could be with using pipes for communication.
That might also explain why we have performance issues with pdfer updating
the display.

I would like to profile sdlscr too. See where it's spending all its time.

Anyway, the interesting things to take away from this profile run are:
 - rendering characters doesn't seem to be the problem
 - constant communication between haskell and C doesn't seem to be a problem
 - The problem is somewhere between sending the updated display and showing
   the updated display.

Let's do sdlscr profiling next. That will hopefuly tell us if the problem is
in drawing the updated display, or waiting for the updated display.

Tue May 10 09:28:52 EDT 2011

As with past attempts, I find profiling sdl doesn't work. It doesn't give me
any time info. Gar! That's so annoying.

The one thing it does say is we can CNSL_GetRed and friends a lot. That is, we
do lots of deconstruction and construction of the colors, which is surely
almost entirely wasted.

The major problem I'm facing here is lack of insight as to what's going on in
my programs. gprof seems not to work so well for sdl programs, and profiling
haskell is always hard for some reason.

Anyway, based on my incomplete information now, I would recommend the
following changes to improve the performance of the termer:
 - Only update that part of the display which has changed
    for my test case usually that's just one cell instead of the whole screen.
    I would expect a speedup of something like 300x then, because we are just
    dealing with so many less pixels.

    This will need work both in ctermer to not update the whole display, and
    in Termer.hs, to not update all the characters.

 - In sdlcsr don't deconstruct and reconstruct the color, make an efficient
   function for converting CNSL colors to sdlcsr colors, namely: the identity
   function.

But the real solution is to figure out how to profile sdl code.

Wed May 11 20:45:27 EDT 2011

The first parts hard. Let me...

Okay, this is sad, but I need to really write a test first of the
communication link.

Here's what I propose. Write an application which switches back and forth
between black and white and black and white. Record how long it takes to do
that for various screen dimensions to try and get an estimate on throughput.

Now, once I have that, make the second optimization with the colors thing, see
if it makes any difference.

This test will be useful for trying out other optimizations to the
communication protocol.

time ./build/src/sdlcsr ./build/src/flicker 640 480 100
hw_available: 0
wm_available: 1

real    0m32.695s
user    0m4.539s
sys     0m26.168s



richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 1280 800
hw_available: 0
wm_available: 1

real    1m35.288s
user    0m14.970s
sys     0m56.615s


time ./build/src/sdlcsr ./build/src/flicker 640 480 100
hw_available: 1
wm_available: 0

real    0m35.451
user    0m11.224
sys     0m24.272

richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 1280 800
hw_available: 1
wm_available: 0

real    1m58.776s
user    0m35.857s
sys     1m13.976s

Some initial results in X and not in X of various sizes.
Hardware acceleration actually appears to slow us down. Or X is speeding us up
or some such.


Now let me try my performance optimization: don't deconstruct and reconstruct
colors. See what that all does to things.

richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 1280 800 100
in X
real    1m54.808s
user    0m12.093s
sys     1m40.698s
 
richard@losaltos:focus$ time ./build/src/sdlcsr ./build/src/flicker 640 480 100
in X

real    0m27.543s
user    0m4.375s
sys     0m22.548s


That didn't make much of a difference.

Sigh. So I have other ideas, such as:
  - fifos are the problem
  - using FillRect instead of just setting pixels directly.

But the right way to do this has got to be enable profiling. So I really
should figure out next how to do that.

Certainly we can reduce the amount of the terminal we update, but for focus in
general, just imagine you are scrolling the screen or a pdf, the whole page
will need to be updated, so we should figure out some way of making that be
fast.

Wed May 11 21:15:54 EDT 2011

Part of the problem is enabling, disabling profiling. Perhaps it is time to
separate consoler out into a library, then start building things with and
without profiling enabled?

I can at least enable profiling on specific applications.

Wed May 11 21:29:45 EDT 2011

Read online about gprof issues. The multithreaded problem with gprof is not
what I'm encountering, because even then we should still be profiling the main
thread. I'm seeing nothing there.

The mac OS X problem is not what I'm encoutering, because I don't have a mac.

The not run long enough problem is not what I'm encountering, because I've run
it for a long long time.

Other things to check: 
 - do we terminate cleanly? If not, we might not get useful output, and I've
   heard sdl might fail to terminate cleanly with gprof.
    No, we are definately exiting cleanly...

 - do non-sdl programs have the same problem? For example, what if I just run
   flicker built with profiling enabled?

    Yes, this works.

So what is sdl doing to break profiling? Probably mucking with timers or some
such.

Anyway, an interesting thing to note: profiling just flicker shows that
flicker is really slow to run by itself.

  %   cumulative   self              self     total           
 time   seconds   seconds    calls  Ts/call  Ts/call  name    
 51.87      0.57     0.57                             CNSL_SendDisplay
 42.77      1.04     0.47                             CNSL_GetPixel
  2.73      1.07     0.03                             CNSL_FreeDisplay
  1.82      1.09     0.02                             CNSL_SetPixel
  0.91      1.10     0.01                             main

The profile suggests that CNSL_SendDisplay and CNSL_GetPixel are slow. I guess
we can't really understand the meaning of that because we don't know that the
interpreter of the data isn't taking a long time. Hmm... what if we pipe the
output to /dev/null? That would assume the server can suck out as much data as
possible, and test how fast flicker can deliver the data. This is an
interesting question.

The answer:

width: 640
height: 480
frames: 100

real    0m7.774s
user    0m2.009s
sys     0m5.762s

Which means... a while.

Another thing I could do: start sdlcsr, and watch top. See which process is
flooring the cpu, which is waiting, and so on.

Top shows sdlcsr as taking 99% cpu while flicker takes 95% cpu. The whole
thing takes about 30 seconds. Not sure if that means anything.

All of which suggests... I'm not sure. That the communication link is slow
certainly...

Well, if I'm really concerned, I could try implementng an alternate version of
ctermer. One whose implementation is directly in SDL instead of using
consoler. If the performance of that is fine, I'll know SDL isn't a big
bottleneck. If the performance of that sucks, perhaps SDL is a problem.

Of course, fbterm is pure SDL and it doesn't have any problems...

I should run this experiment.

Thu May 12 17:01:05 EDT 2011

I made an implementation of termer which uses SDL directly instead of
consoler. It goes noticably faster, but it's still noticably slow.

Which suggests there are many performance issues I'll have to deal with.

Maybe it makes sense to get this version fast before adding the consoler
stuff.

I have the feeling doing FillRect to draw a pixel in SDL is bad. And I think
updating the entire screen for a single character is bad. I propose fixing
those two things to be the next steps in sdltermer.

Unfortunately, it doesn't make sense to do that before I have some sort of way
to evaluate how fast those changes are. I need a benchmark for the terminal
emulator, some nice way to measure how fast it is.

Well... the program I run could be different. Instead of running sh, I could
run a script, it outputs a bunch of stuff (no input), time how long it takes.
That sounds good to me.

Thu May 12 17:17:09 EDT 2011

I made a little test script. It just echos a bunch of stuff to the screen.

On 640x480 pixels, 40x12 lines, fontsize 32, pacific with sdltermer:
 It takes 18 seconds to run.

In contrast, the amount of time it takes to run using sdlcsr...


7m03 seconds. Wow. That's pretty big. 23x

So yes, consoler definately is costly. And now I can do my performance
measurements.

Just for reference, fbcon gets .029s, so I've got a ways to go. Especially
considering fbcon is something I would consider slow.

Thu May 12 20:03:45 EDT 2011

Summary:
 pacific consoler: 243s
 pacific sdl:       18s
 losaltos consoler:121s
 losaltos sdl:       9s

Next task: I have to profile sdltermer. Let me turn on profiling with ghc, see
if that works.

It works! Wonderful. And what it says: 80% of the time is spent in drawCell.

Now, if only I could tell what part of drawcell...

It's either in rendering the character, or updating the sdl surface.

Let me try doing a more memcopy like thing to update the pixels rather than
this fillrect thing, see if that improves anything. The other thing I can do
is cache glyphs. Maybe have a lookup table for the colors...

Okay, make a list. Reasons this might be going slow:
- glyph render is slow
    So cache the glyphs.

    Let me always use the same glyph: say #, which is fairly large.
    And only render it once. Then we'll see how much rendering costs us.

    Um, bad experiment, because now spaces are drawn with '#', which takes a
    long time. Interesting that it takes a long time though, even though we
    aren't rerendering... went from 6 to 10 seconds. Now what if I use space
    instead? Went down to 2 seconds. Notice rending const is not participating
    in these two seconds (or the 10 seconds).
    
    This suggests the biggest cost is copying pixels to the screen surface.
    I wonder what if I don't use a hardware surface? That's seems like a
    worthy experiment.

    It went from 6 to 8 seconds.


- we call it too much
    Higher level concern, not my focus right now.

- Calculating colors is slow
    Maybe use a lookup table.

    I switch to always using 0xFFFFFF, it made no difference at all, so this
    clearly isn't the problem.

- fillrect is slow
    Don't fill rect, lock the surface, do the loop setting pixels, then unlock
    the surface.

    I switch this over, reduced time from 9 seconds to 6. So this is certainly
    significant. 30% improvement.

    Now what if I don't blank the cell backgrounds, is that costing us?
    Drops from 6 to maybe 5 seconds.

    Now, what if I render the glyph, but don't actually update the display?
    It takes 3 seconds.

    And if I now don't render any glyphs? It takes 1 second. Profiling still
    says 30% of the time of the program is spent in DrawCell.


What can I conclude from my experiments?

- copying pixels from the glyph bit map to the display surface is expensive.
- calculating colors cost us nothing
- rendering the glyph takes not an enormous amount of time, but a noticable
  amount of time.

What do I propose to do to fix this?

I propose the following:
 - only update cells whose contents have changed
 - have a glyph cache
    Ideally we could cache CELL sized surfaces, and blit them to the screen,
    which I hope can be the fastest way of getting the pixels there.
    I'm not sure how to deal with the colors though. Use alpha blending of
    some sort to take advantage of the hardware acceleration?

Okay, so I've left it at around 6 seconds. Next step is to only update cells
whose contents have changed. This should make a huge difference.

Question is, how do I implement it?

It would be cool if we could diff two screens. Then keep the previous screen,
keep the next screen, and compare. But that, admittedly, wouldn't be the most
efficient thing to do. I think more efficient would be to have each screen
operation save what cells it changed. We have to generate those lists anyway,
may as well save them.

Or just do the diff. That would be easier. Have a function that takes two
screens, returns a list of (Position, Cell) that have changed from the first
to the second.

Err... but really that's a waste to check each cell if there have been very
few changes, as is most often the case. What if each screen command returned a
list of the (Position, Cell) it changed? Those are exactly what we already
compute.

I feel like this isn't ideal. Maybe we could just cache those changes as part
of a data structure? Like, it would be nice of a screen kept track of recent
changes, and I could clear the changes, and I could get the changes. So add a
function:

recent :: Screen -> [(Position, Cell)]
clear_recent :: Screen -> Screen

Hey, I like that. And I think the implementation can be: save a list of the
Positions which have changed, when they ask for recent, nub it and map that
over cellat to get the cells. 

Cool, let me try this now.

Thu May 12 21:15:22 EDT 2011

I implemented it just like that. Reduces the 6 second runtime down to a little
less than a half a second. Which is quite nice, I must say.

Wonderful. And now I can start using it well enough to make good progress
finding bugs.

Mon May 16 21:42:02 EDT 2011

Performance issues still.

When run with profiling, using the command: top -d .1, we see:

showDisplay takes 26% time, 0% alloc
drawCell takes 24% time, 5% alloc
updatecells takes 20% time, 65% alloc
updatef takes 8% time 9%alloc.

What can I do about this?
Let me brainstorm, then sleep on it.
+ Don't draw cell unless it has actually changed from last time we drew it.
+ Don't keep track of recent updates, but instead perform a diff between two
  screens (possibly based on a recent bounds thing) to figure out what has
  changed?
x Don't keep set of positions that changed, but rather keep a bounding box.
  May reduce memory usage.
+ Read more than a single character at a time from the client
    Perhaps if we read a bunch, apply the bunch, then update the screen, that
    would help things?
+ don't call showDisplay so often. Maybe only after a bunch of updates have
  been made?
- be more precise about what parts of the display have to be updated
  And only update them. Rather than this bounding box thing, which could be
  overkill.
- Track x, y, w, and h as we do cell updates rather than in show display?
- Cache glyphs rather than rerendering every time we call draw cell.
  Maybe blitting to the surface rather than copying pixel by pixel.
- Update cells in some sort of order (if we don't already) to improve caching?
- Use the ST monad to update cells in place rather than allocating memory?
- Move code all into C, out of haskell?


That's quite a few ideas. It would be interesting to see what helps the most.
I suggest I rank them according to which I like best, then go down the line
experimenting with them. I'll definately want a way to evaluate performance to
see if things are getting better. Maybe download an ebook and cat part of that
out. Though the top thing is nice because the cursor jumps all over the place.
It's just bad because it's so nondeterministic. Hmm...

Tue May 17 20:54:06 EDT 2011

I have an idea for a nice test I can run to evaluate performance.
I'll record the output from running top -d .01, or whatever, and play that
back. It exercises lots of different stuff, and in a totally determinate
fashion.

So, let me record something now. Let's say, 30 seconds ish, but without the
time, so maybe I'll record 40 seconds worth of top -d 1. I can always cut it
shorter if I need to.

sdltermer:
    real    0m49.368s
    user    0m38.375s
    sys     0m3.680s

sdlcsr termer (with smaller font)
    real    0m49.225s
    user    0m5.590s
    sys     0m34.840s


Now, to try some optimizations, see if they help.

I feel like it would be cool to do the following:
- Read in a string of characters at a time from the client, rather than one.
- Apply all those changes to the screen
- Only then update the whole screen
    Performing a diff in a single bounding box to figure out what changed.

The trouble here is what if we read a partial sequence, how do we know when to
update the screen? I guess we can keep track of which characters have been
consumed, and wait for a character to be consumed.

That is, we do as follows: The outputters getf function does:
    1. update the screen (if buffer empty)
    2. read a string of characters into buffer (if buffer empty)
    3. return the next character from the buffer (if buffer not empty)

I think we'll update the screen often enough this way: we won't wait doing
nothing without having updated the display, and we won't processes gobs and
gobs of output without updating the display.

This optimization has parts, which should be tried separately.
A. Read characters a string at a time into a buffer rather than one at a time.
B. Using bounding box and diff to figure out which cells need to be updated on
display.
C. Only update the display when the buffer is emptied.

I suppose first I should run profiling.

Profiling says drawCell, showDisplay, and updcells is most costly, so all the
above should be beneficial.

Let me start with (B), because I think it's fairly isolated. In fact, I could
do the whole thing inside the Screen module if I wanted, we just have to keep
a copy of the old... or we could do it outside? There are two ideas. Use diff,
and use a bounding box to speed up diff. Let's just do diff to start.

So you'd like a way to diff two screens:
diff :: Screen -> Screen -> [(Position, Cell)]
    With cursor cells included.

Of course, if the diff happens rarely, then you might be better off, which
suggests doing (C) would be better. But before (C), we need (A). I don't have
time tonight to do any of these. Instead let me investigate how hard it is to
pass a C string from c to haskell.

Well, there's something called a CString, and functions to convert a CString
to a haskell String in IO.

So maybe in my foreign function I return a CString, then convert it. The
CString can be statically allocated on the c side if that would help anything.

I don't know.

Fri May 20 19:01:57 EDT 2011

Here's the first step.

The FromTermClient should return a char*, which is a pointer into a static
buffer, and we should read in a buffers worth at a time.

That will be converted into a String on the haskell side.
A new fromTermClient wrapper will have to be implemented, so add string to the
state monad to make that work.

That's the first step. I would be interested to see how it improves or not
performance.

Let me do some profiling to make measurements, and do a better job saving the
profiles.

Baseline current: a.prof. This is the one that takes 50 seconds.

Now I'll make my string change. Let's focus on the sdltermer implementation.


I made the change. Time now is: 34 seconds. Profile is b.prof.
It was pretty easy to do and works.

Just as a reference: fbterm gets 0.7 seconds, and fbcon 0.5 seconds.

Profiling says getf ends up doing a lot more allocation than fromtermclient
did before. Otherwise the profiles look pretty much the same to me. The
performance improvement is, I suspect, from reading in a buffer at a time
instead of a character at a time on the c side.

Fine. Now what was the next thing I was going to do?

Fri May 20 19:36:16 EDT 2011

Next step: Only update the screen when the buffer is empty.

Will hopefully improve the performance a bunch.

Note, the way the benchmark is set up, the buffer will always be totally
filled, because we have the data ready. In real life interaction that will not
be the case.

I claim this is not misleading, because if the buffer isn't full, there's not
much to do, so performance isn't a big deal. It's when the buffer is full that
I care about performance.

Fri May 20 19:45:42 EDT 2011

Implemented it easy. Now down to 14 seconds, and this time a big change in the
profile (c.prof).

showdisplay, which used to be top time consumer, has dropped a bunch down the
list. Exactly as desired. Let me just double check to make sure the terminal
is still usable like this.

Yup. It certainly is usable still. And much more fluent if I might say so
myself.

Now we are dominated by updcells. For this benchmark practically the entire
cost of that is from the put_char call. We are doing lots of comparisons, so I
bet the set stuff is contributing to this.

Which is exactly the support I wanted to justify my: switch to diff strategy.
We'll figure out which chars have changed by diffing the screen from the old
screen.

So, get rid of recent and clear_recent functions in Screen.
Replace with a diff function.

diff :: Screen -> Screen -> [Position]

Diffing a screen just says which positions are different between the two. You
can then use cellat to get the cells for whichever screen you want. Let's say
the diff does not include any info about the cursor, so the user should deal
with that if he wants.
 
Fri May 20 20:27:34 EDT 2011

Switched to diff. Still takes 14 seconds, so that, I guess, didn't help so
much. Let's look at profiling now...

Profiling suggests things got a little better. We spend less time displaying
cells. The diff is a ways down on the list, so it's not a major contributer to
poor performance.

But updcells is still dominating, even more so now. I think the cost is clear:
any time you put a single character, you end up allocating an entirely new
array and copying all those values in.

Fri May 20 20:35:29 EDT 2011

What's left? How do we fix this updcells issue?

The problem is we put_char a lot, and that is very costly for an array. It's
O(N) for the array.

If I switched to a Map instead of an array it would be O(log N), but then
getting at cells would be more expensive.

No. Let's be really honest. You know what I want here. I know what I want
here. I want to do in place updates. I want just one array I can modify in
place.

That's how it would be done in hardware. That's how it would be done in C
code.

I'll have to change my diff strategy, because that relies on making a copy.
Maybe that's okay. Or maybe not. Let me not worry about that issue yet.
Instead let me do research.

ST monad. Can I make it work for me? And how?

Here's what I read on the ST Monad.

I can runST a monad inside which I have a place where I can make reference. I
can modify these references, and the modifications will be in place (I gather).

So how this could work, for example, is I sit running in ST. I allocate my
array as a STRef, and do my mutations to the STRef, then I don't have to copy
the array.

Wow. The haskellwiki has a page on Arrays. Perhaps there's something better
than ST for me here.

Looks like they have this thing called a DiffArray, which looks and acts just
like an array, but does updates efficiently. Perhaps all I need is to switch
to that. It's still pure (though some yucky hacks I imagine, but maybe don't
need to care about), so I could just drop it in.

What do you think? Is it worth a try? It must be... I'm going to try it.

Fri May 20 21:17:01 EDT 2011

So I switched to DiffArray. It compiles, and tests pass. But it's slower. Now
it takes 60 seconds to do the benchmark.

Profile (e) shows not surprisingly that diff is very expensive now. So now I
should worry about that. How can we make diffs cheap with a diffarray?

What if I had each element of the array be a pair of cells, the old and the
new. Then diff could just compare those, and I would want another function,
cleardiff to update the screen. In other words, go back to the recent,
clear_recent interface. But the difference is I still use diffs, so I still
take advantage of not having to redraw middle characters that haven't changed.
Why will this be efficient? Well, because we only ever do mutations to an
array, we never use the old references. Those go away. The worst case is this
cost to update the old cells when we cleardiff.

Let me try it.

Fri May 20 21:49:27 EDT 2011

Okay, this is better. Now down to 11.5 seconds.

Profiling still says diff is the top problem, though not so bad as before. We
end up allocating a lot less than before we used DiffArrays, but the profile
says we take more time now.

Is there some way I can reduce the cost of diff?

I don't know. I'll think about it some more and get back to you.

