
Sun Jul 31 15:29:29 EDT 2011

Task: alternative to scons?

Scons is much better than make, but it's still a big pain to use.

Complaints about scons:
  - figuring out how to add a builder is complicated
    (though I'm always surprised when I get it working that it works)
  - strange and very annoying stuff happens when trying to copy a directory
    with the same name as a variant directory. I don't understand how it
    works.
  - implicit intermediate files for different environments need to be
    explicitly renamed (thinking profiling versus nonprofiling programs)
  - dependencies must be specified properly and could be wrong (happens
    occasionally)
  - it's too complicated to expect someone else to learn and figure it out.
    *** this is a big point ***

I would like to at least consider some alternatives.

One alternative may be to use scons, but not to do it in the sconsish way.
Write and use my own abstractions and just use the underlying scons mechanism.
That may or may not help.

What I would really like to try (though it will likely have its own problems),
is a tcl based solution which leverages something like strace to automatically
and accuratly figure out all the dependencies for any command you run.

So it will be like tcl, but we remove those command that let you interact with
the world except for some explicit exec command which has been modified to run
the command with strace and figure out the dependencies.

For the first try we could have a model where you always build the entire
project by running all the commands in order, but the tool is clever enough to
sometimes make things go faster by not rerunning things that don't need to be
rerun.

The overhead to learn it? Hopefully none. You just have to know how to specify
your build commands, and you can use all the scriptability of tcl that you
already know and love.

No complicated explicit dependency lists or dependency search algorithms, or
having to worry about such things (perhaps). We may need to make it a little
more complicated to handle certain things (like allowing commands to depend on
generated files), but I could deal with that when I get there, right?

Sat Aug  6 21:41:46 EDT 2011

I want to figure out about how to do this ptrace thing. Maybe try it out on
some commands and such.

Goal: given a shell command to run, run it and output the list of input files
and output files. That is, the list of files read and the files written. In
the future I'll want to do a list of environment variables too probably, but
don't worry about that now.

So! First step, read the ptrace documentation. And maybe strace too.

First question, I suppose, is do I need to use ptrace, or does strace already
have everything I need? Would it be easier to piggy back on top of strace than
to potentially reimplement much of it? I guess the only way I'll know is to
first understand the ptrace interface, to figure out how hard it is to use...

I suspect what I'll have to do is instrument all the system calls related to
files. Or is read and write enough? Or is open enough? Then I'll also need to
instrument the exec and fork family, in order to trace recursively. That
sounds like a bit of work...

Yes. It looks like a lot of work. Hmm...

Maybe for preliminary investigation I should just play with strace, see if I
can't at least see the information I want to see using that.

Okay? That sounds fair to me...

Not sure what to try it on first. Maybe a simple call to gcc would be best.

Sat Aug  6 22:35:22 EDT 2011

Well, here's one possible problem. What about programs like gcc which create
temporary files? The names of the temporary files are certainly likely to
change, won't that cause problems?

Hmm... Sounds like a lot of work.

Tue May 20 22:22:11 EDT 2014

tcl sort of works, without dependency tracking. It's what I use for smten. But
I want to explore a different approach. Why not use autotools?

The benefits of autotools: people are comfortable with them, and they should
work on lots of platforms. Also, it would be good to try, to see if it's worth
doing in general in place of make or scons.

I imagine getting set up with autotools will be a bit of work. A step-by-step
kind of thing.

First questions:
* How do we control what programs/libraries get built and installed?

Here's a review of the component dependency list:

boxer:: none
consoler:: none
documentation:: asciidoc
filler:: none
fonter:: freetype2, fontconfig
green:: none
imager:: libjpeg, libpng
pdfer:: freetype2, fontconfig, poppler-cpp, [poppler-cairo, cairo]
sdlcsr:: sdl
termer:: freetype2, fontconfig

I wonder if auto-tools support components?

Anyway, the idea should be:
 * you can manually disable or enable any component
 * components you don't have the dependencies for will not build
    - but it should be clear somewhere that they have been disabled
 * probably just want to try to build everything by default, disabling what
   doesn't work out of the box.

Well, that's pretty easy.

What are the next steps?

1. Start by setting up the auto-tools to work with the consoler library
and the consolertest case.

2. Then get them to work with boxer.

3. Then get them to work with everything else, one component at a time.

Should be easy. Let me start by researching what we need to get started with
consoler.

I'll target autoconf 69 and automake 1.14, which is what I have installed.

Notes:
 * autotools can be nested. Is that a good thing to do to handle multiple
   components?

Options:
  Recursive Make
    - standard for legacy reasons, not other good reasons.
    - supports conditional subdirectories
  Alternative
    - is not at all well explained in the manual 

Hmm... This is not going to go as smoothly as I like, I suspect.

People suggest cmake as the most decent alternative, but it doesn't appear to
be up to snuff from what people say.

So it's worth trying autotools.

Let me not do recursive stuff?
I don't know.
Let me just do whatever works.

Wed May 21 16:04:17 EDT 2014

Let's set up things with consoler and see what goes wrong.

It might be a little messy to start. Don't worry about that.
I don't think each component should be treated as a separate thing needing
configuration.

Questions:
* How to do a library instead of a program?
  Okay. Looks simple enough.

* Do I need to specify that my test case needs to be linked with the library I
  built?
  
  Let me try and see.

I think I have the basics in place for consoler with the test case.

Let me try building, and see what happens.

It works! Cool. That's nifty.

Next steps:
* Figure out how to do the pkgconfig for consoler
   Looks like:
    - I specify foo.pc.in
    - I use @prefix@ to refer to that
    - I add foo.pc to the autoconf AC_OUTPUT list.
   Or perhaps:
      I add foo.pc to an AC_CONFIG_FILES([foo.pc]) macro
   Oh. That's what I meant. I get it.
   Now, in order to install it?
   Poppler does:
        pkgconfigdir = $(libdir)/pkgconfig
        pkgconfig_DATA = foo.pc
   Why does this make sense?

   Oh. Maybe it's: foo_DATA gets installed in foodir.
   So by defining the directory, that then makes sense.

   Yes. This is what the automake manual says to do. Good.
   And it works. Cool.
    
* Figure out how to do the documentation for consoler (could be tricky) 
   I could try texinfo. I think that would be worth while.
   And man.
  
   But the thing is, I want a pre-processor.
   That is, a single common specification for the documentation of each
   function.

   I like having this placed in the header file, because it makes sense to go
   with the header file. If you change the header, you should change the
   documentation.

   This suggests I'll want a tool which reads the header file and extracts the
   documentation. Then generates the man file or the texinfo, or input texinfo
   files. This is what we currently have.

   The question is:
   * how to invoke this preprocessor from automake.
   * what the preprocessor should look like in order to be nicely portable
   * what the preprocessor should generate for the man and texinfo files.



* Figure out how to have boxer include consoler.h?
   I would like to include it as #include <consoler.h>
   I see no special INCLUDES thing in automake.
   So perhaps just use CPPFLAGS with -I../consoler

   Yup. That works fine.

* Figure out how to do the boxertest, which needs boxer to work.
   We could assume boxer is in the current directory?
   Doesn't that seem reasonable to you?

* Start flushing out the rest of the components.

Fri May 23 16:12:54 EDT 2014

Next question: How to find the include directory for freetype?
And also ensure freetype2 is installed?

I should be able to do a pkg-config check.

Fri May 23 19:17:40 EDT 2014

Questions:
 * how to build documentation?
 * for filler: how to find tcl.h?
 * what about all those posix things I use in green and such?
   Shouldn't we have checks for them?
 * how to conditionally build components that have dependencies I may not
   want?
 * how to output a nice summary of what will be built (once we support
   conditionally building things
 * is there a problem with dependency tracking?
    - I changed pdfertest.c, but it didn't recompile it. Why?
      It's like all my nightmares from make all over again.

I don't like how every command is built by linking against every library.
Shouldn't it be per command?

Sat May 24 14:41:30 EDT 2014

Ignoring the issue with tcl.h for now, I now have all of the executables
building, and all of the test cases running and passing.

This leaves the following items before I start to think about how to clean up
the build system:
 * man pages
 * documentation
 * screen term info file.

I think term info file should be the most straight-forward.
First, figure out what I'm supposed to do:
 - where should the file go
 - should I install it by running tic?

Sat May 24 14:53:36 EDT 2014

The existing sconscript build system doesn't do anything with the term info
files. So let me not worry about them now, but rather, later.

So then, it's on to documentation.

I think this is worth its own task. Say 72.tsk.

Mon May 26 10:11:09 EDT 2014

I have decided to punt on 72.tsk for the time being. Use ascidoc for the
documentation. Figure out how to support it in the build system.

Let me start with the boxer man page.

Inputs:
  boxer.1.txt - the asciicod description of the man page.
  boxer.txt - the description included by boxer.1.txt.

Goal: generate boxer.1
How:
 a2x -v -f manpage -a VERSION=$(VERSION) boxer.1.txt

Then, I would like to use the generated boxer.1 file in a standard automake
man page rule.

Okay, how do I add this rule for asciidoc to automake?

Looks like just like in a normal makefile.
Let me try it.

Mon May 26 10:23:45 EDT 2014

That works swell. The real question will be figuring how to do the right thing
if the user doesn't have asciidoc installed.

And also, how to avoid replicating the rule in every makefile.

Mon May 26 10:46:40 EDT 2014

Ug. And now I feel like we are back to the original problem: I don't want to
use make for the build system.

I suppose all we really need is a configure file.
In order for people to feel happy about building things.

We don't actually have to use autotools to make it.

The thing that is nice about autotools is they know about the standard things
people want to do. Annoying things like: what flags people expect to be able
to give, and how to build in a separate directory, and how to do parallel
make, and all that fun stuff. How to build shared libraries.

Can I make my own autoconf macros to make it so it doesn't look like I'm
reverting to make given that automake doesn't know how to handle asciidoc
files?

What is it that I want?

Okay, here's what I propose.
Do the yucky manual copying of rules thing.
Get everything to build and install (assuming you have everything you need).

Then, I can start cleaning things up, and trying to make them better.

Next step:
  All the rest of the application man pages.

Mon May 26 11:04:38 EDT 2014

What remains now:
 * library man pages
 * the top level documentation (via asciidoc)

Question: can we generate texinfo from asciidoc?

It looks like no.

The next best thing: can we tell automake about the html to be installed?

Maybe just another rule, and it automatically gets installed?
Or I can install it as dist_DATA I suppose.

Mon May 26 11:17:27 EDT 2014

Fine. It's ugly. I basically reverted to make.

And it's like... what's the purpose of automake at all if you are just going
to revert to make?

Anyway. Next step: generating all the library man pages.

Ug. This is so complicated. Because I don't know the generated files
statically.

Mon May 26 14:13:33 EDT 2014

I'm not happy with autotools if it just reverts to make.
It's already had problems with dependencies. I already don't trust it, and end
up git cleaning everything and rerunning everything.
This makes it a failure in my mind.

What do I want from a build system?

There are two distinct users of a build system:
A. Developer: Goal is to develop an application.
Developers will often make small changes, and want to quickly rebuild the
entire system based on those small changes.

B. End user: Goal is to build the application and install it.
They have not previously built it. They don't care about tracking
dependencies, because they just want to build it once. That is, unless the
build fails, in which case they do care about dependencies, because they don't
want to have to recompile everything to try and fix it. And let's be honest,
builds fail a lot.

There is also a configuration question: What programs and utilities and
libraries are available on the current system. What does this mean for how to
build and install. What does this mean for *what* to build and install.

There are two ways for a user to help find needed programs utilities and
libraries: guess where they are and see if we are right, or ask the user for
them. In practice, probably best to guess, but let the user overwrite things
if we are wrong.

What's wrong with make and automake and scons? A human is responsible for
specifying the dependencies. These dependencies are inevitable incorrectly
specified, meaning we can't rely on partial rebuilds. In this case, may as
well just have a single script to build everything always.

This doesn't entirely work out for, say, smten, because of two things:
1. It's really too much of a pain to rebuild everything always.
For smten in particular, having to download and install all the cabal
dependencies is unreasonable.

2. Haskell tries to do its own partial rebuilds, and it messes up.
Stupid.

Thus, my conclusion, if we want to have a decent build system with support for
partial rebuilds, we must automatically determine the dependencies.

How could you automatically determine dependencies?
 * Use system call traces
 * Restrict the language to one which tracks things.

The second option is not satisfactory, because it requires anything you want
to use for building to be implemented in the language.
But in practice we must support existing tools and commands and languages for
building things.

So, assuming we use tracing somehow to figure out what everything does.
The only way to know the dependencies is to run it. That sounds right to me.
This means that the first time we build the system, we don't know the
dependencies. What order should we build it in?

I believe there should be a clear order in which to build things.
Is that the case?
Like, the order I would write if I just wrote a shell script to build
everything.

This order may be overly restrictive: it may not allow parallelization and
other things we would like to improve performance if we knew all the
dependencies. But note, this order is also probably fine if all we want to do
is build and install the thing, without partial rebuilds.

Okay, so far we have the following:
 * You specify build commands using arbitrary executables
    (shell script, other script, makefile, compiler call, whatever)
 * You specify an order to build things the first time.
    A sequence of build commands.

What other things are there to worry about?
 * Configuration
 * Cleaning
 * Phased builds
    - to let you build only those parts of the graph named.
    I'm thinking like:
    The maintainer may want to build part of the package before distributing it.
    Or you may want to 'make check' separately from 'make'.
    And 'make install', etc...

Sun Jul 13 16:45:50 EDT 2014

It's also important for people to be able to configure and build the package
in a standard way.

Thus, the interface should be the standard interface:

 ./configure && make && make install

With standard make targets.

The job of configure is to figure out what should be built, and where
libraries can be found.

Note: just because we call make and support the make targets, does not mean we
need to use make for the core build process.

Now, then, thoughts on tracking dependencies, and when it is safe to not
compile something again...

Things which affect a command are the environment, and command inputs.
Let's assume the only way to change the command inputs is to change the
environment.
What things in the environment can effect the build?
Or, in other words, what operations can the commands do to access the
environment? System calls.

* presence or absence of file
* contents of file
* presence or absence of env var
* content of env var
* and many, many other possible things

But the idea is clear: 
We keep a database for each command, when we first run it, we record all the
environment queries it made.

Next time we go to run the command, we check if the environment is the same or
not. Note: we probably want to do a bunch of caching to see.

I would expect, for the most part, the first time we run the command, it
obviously must be run. The second time we run it, we probably want to run it
again too, because it generated things from the first time. After that, I
think we will have reached a steady state.

There is also some dependency grahing to do. Things like: this command
doesn't depend on this other command, so they can be executed in parallel.

Good. There is a lot of work to do to implement this, in terms of tracking
what things do, making sure we don't mess things up. But I should be able to
implement a naive version which just always reruns things to start, and slowly
improve performance. The key question is: what should be the interface?

We want primitive commands - so you can call the primitive build however you
want.

We want command sequences - a conservative description of command
dependencies, which we will be able to automatically refine.

Anything else?

How about a general purpose language for commands? Something like tcl?
To make it much easier to describe these primitive command invocations and
command sequences.

How about some way to call commands hierarchically? To allow for finer grain
parallelism? Or does that not really matter?

Something we will have to assume is that the environment will not change
externally during the execution of a command. At lesat, the important part of
the environment.

Well, I think the interface to use is obvious: write tcl scripts. But redefine
the core tcl commands to do dependency things.

Can I do that? Can I recognize a sequence of commands in tcl and then run
them? Yes. I can change 'exec', for example, to record the list of commands.
Except, we might want some way to refer to the output... That's awkward.

I want to limit the language, because I think it will make life easier.

But, anyway, let me start by assuming the full language, make an initial build
script, and see what all kinds of things I want to specify and how.

Cool. So, plan is: ditch autotools, ditch scons, write a tcl script to build
focus. That will tell me what things I need, how I want to write the script.
It will be annoying to build, because it will always rebuild everything, but I
should be patient. That's the key.

Cool. Let me get started then.

Sun Jul 13 17:17:39 EDT 2014

What to start with?

Consoler.
I'll have a file called make.tcl at the top level, and then one in each
directory, and they can source each other. What do you think about that?

Sounds reasonable to me.

Sun Jul 13 17:36:48 EDT 2014

Question: I want different make commands. How should I support those?

Well, the obvious way would seem to be: have a different script for each
command. They can share common stuff if they need to.

Cool. So next step is: make check.

But this suggests we might have a lot of files for making. So, what if I put
them in a directory called tclmk? Or have some way to make it clear they are
part of the build system and nothing else? Hmm... Something to ponder.

Sun Jul 13 17:54:19 EDT 2014

Actually, it doesn't work so cleanly, because there are dependencies.
Check should run normal make first. But not complete normal make.

Perhaps a better way is to have conditional commands:
* if CHECK, then do the check stuff
* if INSTALL, then do the install stuff.

Yes. I think that makes more sense to me.
Better to keep things ordered and all together.

Tue Jul 15 18:49:48 EDT 2014

Question: How to do make clean?
Shouldn't this be automatically generated based on what the commands produce?
Or should I manually specify it?
That's a little bit annoying.

Also, if we make clean, we don't also want to build things first.
That would be silly.

Tue Jul 15 21:23:32 EDT 2014

Goal: complete the build script, so it builds everything and installs
everything as it should. Once that is done, I can ask about how to do
configuration, and all that fun stuff.

What things are currently missing?
I think the librarydoc thing: I need a script to extract the librarydoc
comments.

I suppose I could use a python script.
I think it makes more sense to use a tcl script.
I'll just need to look into how it works.

