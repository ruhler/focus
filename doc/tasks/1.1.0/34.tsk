
Sat Jun 25 13:34:24 EDT 2011

Task: Fix concurrency issues in screen.

I don't do anything to guarantee the atomicity requirements in screen are met,
so there are race conditions and the bugs corresponding.

I've seen in the field screen randomly seems to crash, and I suspect this is
the problem.

I need to fix this before adding the split feature.

The way to fix it, I think, is to provide some transactions that I can use to
manipulate the screen structures. So like a library.

I'll want the following transactions I think:

- switch current window
Input: window to switch to.
Result: current window is switched if that window exists, otherwise nothing
happens.

- redraw current window
Input: dimensions to redraw

- update a window
Input: maybe a client? Should this be blocking? Then how could it be a
transaction?

Some thought is needed for this one.

- close window
Input: window to close
If it's the current window, it should select a different current window.

- new window
Input: client of the new window.

- send event
Sends an event to whichever client is the current window.


All the main data structures should be protected by a lock. I think one lock
is fine for now. I don't expect us to run into much contention.

The server stuff should be the user of this library. Maybe call the library
green and the client still cgreen and the server, which uses the green
library, sgreen.

Fri Jul  1 15:56:21 EDT 2011

Okay, I want to work on this now. I'm not sure why it seems so hard.

I considered not making this a multithreaded application. It could use poll
where it's polling on input events and display updates from children and new
client requests in one big loop and then no have any problem with race
conditions ore anything like that.

That's probably the smart thing to do. What's the problem?

There's this idealogical problem that we should be able to have threads and
use them, and the only reason we don't is because the interface to use them is
so terrible.

The main concern about the polling is to do it right you have to maintain
intermediate state for each request. You can't say: oh, a client has a display
update, and wait for the whole update. Because what if the client never gives
you the update? Then you're blocking everyone. The protections aren't right.

So let me try again with threading. The general strategy is, I believe, clear.
We have a thread for processing inputs. We have a thread for accepting client
requests. We have a thread for each active client which is handling display
updates from it.

To be safe, everyone will be accessing a shared variable which is
"thread-safe" as they say. It provides a bunch of atomic updates.

Good. So the first question then, is what should this data structure be, and
what atomic actions should I provide for working with it?

We can cap the number of clients to 10 or something. I don't mind that at all.

So we'll have a fixed number of client slots. Zero through nine. For each slot
we need to know if there is a valid client there. If there is a valid client,
we will want its CNSL_Client handle. We also want access to the CNSL_Display
for that client. It should be the right dimensions, so lets say we dynamically
allocate the displays along with the clients. This shouldn't be a problem,
because launching new clients is definitely a rare operation.

We need to know the currently active client.

I think that's all for the shared state. So it will be:
  (client_id, Array (Maybe (Client, Display)))

Something that is worth asking is, how do we shutdown? What is the shutdown
procedure? Who ends the program?

Shutdown normally comes when the last window closes. This will be discovered
by the thread for the client which closes last. That doesn't help much.

We have to do cleanup: clean up the server socket, shutdown the server.

I'm not sure. It's messy. Maybe we can have a way to ask for the current
number of clients. When the current number of clients is 0, then whoever wants
to quit can quit, and that thread needs to make sure it cleans up right. I
don't know. Hopefully I can deal with that later.

Good. Now I have the plan, a data structure. I don't know how to end things,
but we can save that for a later date to get right. Next is to figure out how
to proceed.

How about this. I'll start writing my Green class which is the thread safe
interface to the shared data structure. Then I'll try to port sgreen.c over to
using that class, and add to the class as appropriate?

No. Better to really figure out how things should work and reimplement it all
from scratch.

I can do this. We have the following threads:

input - receive input and forward it to the current client.
    This also handles green commands like switching windows.
    It would use the following atomic actions:
        - send an event to the current window
        - switch to a given client
        - add a client

server - listen for new client connections.
    - add a client

output - get output from a client and update the display if needed.
    It needs access to the client display, ideally in a way where it is safe
    to block (receiving the display may take time, but we still want it to be
    done directly).

    Perhaps the local display for the client should be local to the thread
    handling the output. Then all it needs from the shared data structure
    is...

    - update display
        Given client id and local client display, and dimensions, update the
        primary display if that client is currently active.
    - remove client

    One wonders, if the client local display can be local to the thread, can
    the client handle too? The input from the client handle will be owned by
    the outputter, but the output to it will be owned by the inputter. Or
    whatever the right names are. To correctly handle when the client opens
    and closes, it makes sense to manage these in a shared atomic way.

Cool! I think that's everything I need.

Shared Data Structure:
  (current, Array Client) - note no displays here

Atomic Actions:
    - send input event to current client
    - switch to given client
    - add a client
    - remove a client
    - update the display if current client

I think that's plenty to start coding.

Fri Jul  1 17:13:32 EDT 2011

Ack! Trouble. Bugger.

Here's the problem. When you close a client we need to switch to another
client. To switch to another client, someone has to update the display with
that clients display. But the only person with the clients display is the
thread where that display is local. I do not want to wait for that client to
happen to have a full display update, which probably will not happen.

So something needs to be fixed here. Hmm... let me think about this a little
bit and get back to you.

Fri Jul  1 19:18:21 EDT 2011

Let me list the problems I'm facing. It's not pretty.

lock/unlock pairs are annoying, because, for example, I can't return out of
a for loop. It would work better if I wrote my functions independent of the
locks, then wrapped each one in lock/unlock pairs, but in C wrapping functions
like that is not convenient.

The real problem seems to be that consoler does not support any way to perform
an atomic update of a display. The update could always block indefinitely. We
want something that says: if you run this, and someone else wants to use the
display at the same time, if this will not finish in a reasonable amount of
time, the other person who wants to use the screen can. This could perhaps be
implemented if I duplicate state: load into a temporary buffer, then
atomically switch that with the front buffer. I'd rather avoid the cost of
that duplication though (it's significant, right?).

Well, that's something I should maybe think more about. Perhaps I'm too caught
up with performance? Though I do have performance problems... I'll come back
to this thought later.

My original proposed solution to solve this non-atomicity problem is to
pretend it doesn't exist. Assume once you start to read in a display, the rest
will come quickly after. If that's not the case, you have a slow connection or
a malicious program or something. Gar. Bugger. I'm not sure this a fair
solution.

But assuming it is fair, there is still the requirement that we know the
difference between: I'm blocking waiting for an update to be sent, and: I know
an update has been sent, and I'm blocking waiting for the update to complete.
That needs some sort of poll call. I removed my CNSL_PollDisplay function
though.

If I go the non-multithreaded route, the poll is pretty obvious, because I
need to do a poll anyway. If I go the multithreaded route, I guess I can still
use poll. Then each display should be gaurded with a mutex which is locked
once poll says something is available and unlocked once the update is
complete.

Deep down I feel like the non-multithreaded route is wrong. You have to know
about everything that's going on. You can't reuse functions which weren't
stupidly chopped up into lots of little pieces which let you do explicit
scheduling. No. Multithreaded has to be the right way of it.

If I don't lock the display in a multithreaded application, and I close a
window at the same time the one I switch to is being updated, the screen may
be corrupt, and that won't be fixed until the corrupt part of the screen is
updated, which may not be for a long long time, and we don't currently have a
way to force the update.

Well, that brings me back to what is perhaps the right option. The option to
explore at this point.

We need an atomic update display function. We can do that with multiple
buffers.

So here's what we do. Each client output handler thread has a local copy of
the client display. It can do whatever it wants with it.

The green object then has an additional display for each client, and a
function to update that display from another display (blit) atomically. So
once a client output handler thread updates its display, it can then
atomically update the display for the client in green (which will lock things
up), it will then forward the update to the main display if the client is
current.

Lots of copying. That worries me. Isn't that a waste?

What about the other option. Fixing the non-multithreaded version. Implement
partial display updates. No. That's yucky.


Well, so that's the answer then I suppose. If need be we can deal with the
performance issues as they arrive. We can introduce finer grained locking. I'm
not sure how we can avoid all this extra copying, but maybe it won't be so
bad?

The typical case is when there is one client sending displays. 

Annoying. It's annoying. I fear performance problems. I don't understand why
I should have to suffer these potential performance problems.

Oh well. I'm sick of the bugs I encounter when using sgreen. I want to
implement something that should work right with no known possible race
conditions or bad stuff to mess it up.

The conclusion: Try implementing the double buffer thing above, take the
performance hit. If it's really a problem, I'll have to worry about how to
improve the performance later. Removing bugs at the cost of possibly some
little performance loss is an improvement. Moving bugs from one place to
another is not really an improvement.

Okay. So that's decided. Now how do I implement this?

We add displays to the green object state. Whose job is it to allocate them?
When we add a client we can allocate them. How do we know how big to allocate
them? Well, I know for this version all the windows will be the same
dimensions, so I don't really have to worry about it. Let's say, um, that, um,
when you add a client, it automatically allocates a display based on the
consoler geometry.

We can reuse the same SendDisplay function I already have to update the
non-local displays.

Okay. Good. I'm glad that's settled. Let me update things and forge ahead.

Fri Jul  1 20:21:22 EDT 2011

I finished the rough draft of my shared object library. I would like to note
again what's annoying about these locks: in order to compose things nicely I
would have to implement both locked and unlocked versions of each of the
functions, which is slightly annoying to write down in c.

Next step is to update sgreen.c to use the library I just wrote. Let's see how
many other issues we run into...

Or maybe I should plan it out first.

The main function reads in the command line arguments, which could get us our
socket name. It initializes a green object, which we can keep a global pointer
to (same with the socket name perhaps).

We should start a shell client and add that to the client list. This will
spawn the handler thread for it. I should spawn the server thread. Then I
should handle input. In all this, who do we deal with the end condition? I
need to figure that out now.

We can stop if we get a quit event, in which case we should probably forward
it to all the valid clients, right? We can also stop if the last client is
removed, so I should add a way to check if there are no more clients. So the
inputter can deal with this, it can run on the main thread and know when to
finish, and clean up after the server.

Fine. I think I'll want a function which takes a CNSL_Client and spawns the
handler thread for it. The handler thread can allocate its local display, add
the client to the green object, and update the initial display which is filled
black.

Good. Now I'm ready to forge ahead. I think I'll just hack up the existing
sgreen code.

Fri Jul  1 20:58:18 EDT 2011

Finished the rough draft of updated sgreen.
You think it will work? I don't think so, but we'll see.

Fri Jul  1 21:13:52 EDT 2011

Surprise surprise. Doesn't work. Looks like a bug. I'm probably not using the
mutexes right, which is kind of hard to do given I don't have any
documentation for how to use them.

Fri Jul  1 21:28:36 EDT 2011

There was a race condition for the first client starting. So I changed so
sgreen won't quit until the first input is recieved.

Now the test passes, but... somehow I'm skeptical, because it didn't seem to
pass cleanly the first time. Let me try again.

Here's the problem: the sgreentest finishes, but the child process still runs
on and on. It doesn't actually quit. How can I test for that? I don't know the
pid of the launched thread.

Fri Jul  1 21:39:50 EDT 2011

For some reason sgreen isn't terminating. It's getting a slew of quit
requests, which I would expect to cause all clients (namely, filler) to quit,
causing the last client to be removed, and thus ending. Why does that not
happen?


Oh. It quits. It just takes a little time first. Okay. I'll accept that.

Fri Jul  1 21:47:03 EDT 2011

I think maybe it works! I'm going to switch to this version, start using it,
and I'll report any bugs we encounter. Until then, on to split I suppose.

